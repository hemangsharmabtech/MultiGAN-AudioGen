
```markdown
# VQ-VAE for Voice Conversion ğŸ§

This repository contains a Vector Quantized Variational Autoencoder (VQ-VAE) implementation tailored for voice conversion tasks. It leverages Mel spectrograms and MFCC features for learning discrete latent audio representations.

---

## ğŸ§  Architecture

- **Encoder**: Compresses input spectrograms into a latent space.
- **Codebook**: Vector quantization of latent space to discrete tokens.
- **Decoder**: Reconstructs audio features from quantized representations.
- Trained with **reconstruction loss**, **VQ loss**, and **commitment loss**.

---

## ğŸ“ Directory Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mel/                # Mel spectrograms (.npy)
â”‚   â””â”€â”€ mfcc/               # MFCC features (.npy)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ vqvae_encoder.pth
â”‚   â”œâ”€â”€ vqvae_decoder.pth
â”‚   â””â”€â”€ vqvae_codebook.pth
â”œâ”€â”€ train_vqvae.py
â”œâ”€â”€ inference_vqvae.py
â”œâ”€â”€ vqvae_model.py
â””â”€â”€ README.md
```

---

## ğŸš€ Training

```bash
python train_vqvae.py
```

This will save the encoder, decoder, and codebook separately under `models/`.

---

## ğŸ“¦ Loading Model Components

```python
import torch
from vqvae_model import VQVAE

# Define input dimension (e.g., mel + mfcc)
input_dim = 93
model = VQVAE(input_dim=input_dim)

# Load weights
model.encoder.load_state_dict(torch.load("models/vqvae_encoder.pth"))
model.decoder.load_state_dict(torch.load("models/vqvae_decoder.pth"))
model.codebook.load_state_dict(torch.load("models/vqvae_codebook.pth"))

model.eval()
print("VQ-VAE model components loaded successfully âœ…")
```

---

## ğŸ“Š Dependencies

```bash
pip install torch numpy matplotlib tqdm
```

---

